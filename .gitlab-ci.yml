# TEMPLATE GITLAB-CI FOR CCAO MODELS

# This CI script creates the R environment to run the CCAO's automated valuation
# models. It uses the same basic pipeline as the dvc.yaml script included with
# the model, but excludes certain outputs and stages. It also runs each stage in
# an independent GitLab runner

# This setup uses dependency caching to speed up build times, see below for
# more details

# Base R image to build the model with. Should be version locked to the highest
# version most CCAO employees are using locally
image: rocker/r-ver:4.2.2

# Defining static variables used throughout the CI pipeline
variables:
  DOCKER_DRIVER: "overlay2" # Docker FS driver, don't change this
  GIT_SUBMODULE_STRATEGY: "recursive" # Set how GitLab handles submodules
  # If a package has a linux dependency that isn't already listed, add it here
  APT_DEPS: "libcurl4-openssl-dev libssl-dev libxml2-dev libgit2-dev git libudunits2-dev python3-dev python3-pip"
  # Specify vars to enable caching and dependency management via renv and pip
  RENV_CONFIG_REPOS_OVERRIDE: "https://cloud.r-project.org/"
  RENV_PATHS_CACHE: ${CI_PROJECT_DIR}/cache
  RENV_PATHS_LIBRARY: ${CI_PROJECT_DIR}/renv/library
  PIP_CACHE_DIR: ${CI_PROJECT_DIR}/cache 

# Cache settings. R libraries are installed into the renv cache based
# on the libraries listed in the renv.lock file. These libraries are then
# copied between each build as a .zip file. This mitigates the need to reinstall
# libraries for every build (which takes a long time). The cache will be re-used
# until the renv.lock file changes.

# The input data is also versioned using DVC. If the dvc.lock file changes, then
# the cache will be busted and new data will be pulled from S3
cache:
  - key:
      files:
        - renv.lock
    paths:
      - .apt
      - ${RENV_PATHS_CACHE}
      - ${RENV_PATHS_LIBRARY}
  - key:
      files:
        - dvc.lock
    paths:
      - ${CI_PROJECT_DIR}/input

# Run all of these commands before starting any jobs
before_script:
  # These are commands for caching installed APT_DEPS, they slightly speed up
  # overall build times but aren't strictly necessary
  - rm -f /etc/apt/apt.conf.d/docker-clean
  - mkdir -p .apt && mkdir -p /var/cache/apt/archives && mount --bind .apt /var/cache/apt/archives/

  # Install apt dependencies listed in APT_DEPS variable
  - apt-get update && apt-get install --no-install-recommends -y ${APT_DEPS}
  
  # pip install DVC to fetch data from S3 using dvc pull
  - pip install aiobotocore[boto3] boto3 dvc[s3] 
  
  # Install R dependencies listed in renv.lock using renv
  - Rscript -e 'renv::restore()'
  
  # Pull any missing input data
  - dvc pull

# Each of the stages listed below corresponds to a script in the pipeline/
# directory. The purpose of each stage is described in dvc.yaml. Artifacts are
# used to pass objects between stages. They expire quickly because they're
# typically uploaded to S3
.job_template: &run_stage
  artifacts:
    paths:
      - output/
    expire_in: 6 hrs
  only:
    - master
    - /^[0-9]*-assessment-year$/

# Note that the assess and interpret stages are skipped because they take too
# long on shared GitLab runners. Therefore, each CI run produces metrics on the
# test set only
stages:
  - train
  - evaluate
  - finalize

train:
  <<: *run_stage
  stage: train
  tags: 
    - saas-linux-large-amd64
  script:
    - Rscript pipeline/01-train.R

evaluate:
  <<: *run_stage
  stage: evaluate
  tags: 
    - saas-linux-large-amd64
  script:
    - Rscript pipeline/03-evaluate.R

finalize:
  <<: *run_stage
  stage: finalize
  script:
    - Rscript pipeline/05-finalize.R
