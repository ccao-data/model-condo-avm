# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
##### Setup ####
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Load R libraries
options(tidymodels.dark = TRUE)
library(arrow) 
library(assessr)
library(beepr)
library(ccao)
library(dplyr)
library(here)
library(purrr)
library(recipes)
library(stringr)
library(tictoc)
library(tidymodels)
library(treesnip)
source("R/model_funs.R")
source("R/valuation_funs.R")

# Start full script timer
tictoc::tic(msg = "Full Valuation Complete!")

# Set seed for reproducibility
set.seed(27)




# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
##### Valuation ####
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Load the final stacked model object from file
final_full_fit <- model_load(here("output", "models", "stacked_model.zip"))

# Get the most recent arms length sale for each property in the training dataset
sales_data <- read_parquet(here("input", "modeldata.parquet")) %>%
  group_by(meta_pin) %>%
  filter(meta_sale_date == max(meta_sale_date)) %>%
  select(meta_pin, meta_sale_price)

# Load the full set of residential properties that need values, join the most
# recent sale and fill all missing sale prices with 0. This is a workaround 
# to prevent NA sales values from being removed when creating predictions
assmntdata <- read_parquet(here("input", "assmntdata.parquet")) %>%
  left_join(sales_data, by = "meta_pin") %>%
  mutate(meta_sale_price = ifelse(!is.na(meta_sale_price), meta_sale_price, 0))

# Get the subset of PINs that actually can be valued by the model (no missing
# or strange data). Then bind the predicted values generated by the model object
# to those IDs. This step is necessary because PINs that can't be modeled are
# removed via the prepped recipe, but ID columns can't actually be passed
# through the model (else they're included in the design matrix)
assmntdata_preds <- bind_cols(
  bake(final_full_fit$recipe, assmntdata, has_role("id")),
  list("lgbm" = model_predict(
    final_full_fit$spec,
    final_full_fit$recipe,
    assmntdata
  ))
) %>%
  mutate(meta_pin = str_pad(meta_pin, 14, "left", "0"))

# Join predicted values back onto the assessment data and cleanup
assmntdata <- assmntdata %>%
  left_join(
    assmntdata_preds,
    by = c("meta_pin")
  ) %>%
  mutate(
    meta_nbhd = str_pad(meta_nbhd, 3, "left", "0"),
    meta_sale_price = na_if(meta_sale_price, 0)
  )

# Remove unnecessary objects
rm(assmntdata_preds, sales_data)




# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
##### Post-Valuation Model ####
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Create post-valuation model object that saves aggregate adjustments
pv_model <- postval_model(
  data = assmntdata,
  truth = meta_sale_price, 
  estimate = lgbm, 
  class = meta_class,
  med_adj_cols = c("meta_town_code", "meta_nbhd")
)

# Save postval model to file so it can be loaded later for prediction
pv_model %>%
  saveRDS(here("output", "models", "postval_model.rds"))

# Get adjusted values for assessment data 
pv_final_values <- assmntdata %>%
  mutate(
    meta_town_name = town_convert(meta_town_code),
    final_value = predict(pv_model, ., meta_sale_price, lgbm),
    prior_value = meta_est_land + meta_est_bldg
  )

# Save final values to file so they can be uploaded to AS/400
pv_final_values %>%
  write_parquet(here("output", "data", "finalvalues.parquet"))

# Generate valuation diagnostic/performance report
rmarkdown::render(
  input = here("reports", "valuation_report.Rmd"),
  output_file = here("output", "reports", "valuation_report.html")
)

# Stop full script timer
tictoc::toc()
